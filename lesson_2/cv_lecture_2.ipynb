{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "854e389e",
   "metadata": {},
   "source": [
    "# Лекция: Компьютерное зрение\n",
    "## Классические алгормимы компьютерного зрения\n",
    "\n",
    "**Преподаватель:** Весельев Александр, Т-Банк\n",
    "\n",
    "**Аудитория:** ФКН ВШЭ, МФТИ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e064754",
   "metadata": {},
   "source": [
    "## Feature Detectors and Descriptors\n",
    "\n",
    "### Harris Corner Detector\n",
    "\n",
    "Способ найти на картинке *углы*: места, где изображение меняется сразу в двух направлениях (и по X, и по Y).\n",
    "\n",
    "*Идея*: при небольшом сдвиге окна (патча) вокруг точки:\n",
    "\n",
    "- в угле картинка плохо совпадёт при любом маленьком сдвиге\n",
    "- на ребре – совпадёт при сдвиге вдоль ребра\n",
    "- на ровном – совпадёт почти всегда.\n",
    "\n",
    "![](./assets/lesson_2_image_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3892fb",
   "metadata": {},
   "source": [
    "#### Функция энергии\n",
    "\n",
    "Пусть изображение – функция яркости:\n",
    "$$I(x,y)$$\n",
    "\n",
    "Берём маленькое окно $W$ вокруг точки $(x,y)$ и смотрим, насколько изменится картинка, если сдвинуть окно на $(u,v)$:\n",
    "$$\n",
    "E(u,v)=\\sum_{(i,j)\\in W} w(i,j),\\bigl(I(i+u,,j+v)-I(i,,j)\\bigr)^2\n",
    "$$\n",
    "где $w(i,j)$ — веса окна (gaussian), чтобы центр был важнее.\n",
    "\n",
    "#### Линеаризация\n",
    "\n",
    "Для малых $(u,v)$:\n",
    "$$\n",
    "I(i+u, j+v)\\approx I(i,j)+I_x(i,j),u + I_y(i,j),v\n",
    "$$\n",
    "где\n",
    "$$\n",
    "I_x=\\frac{\\partial I}{\\partial x},\\quad I_y=\\frac{\\partial I}{\\partial y}\n",
    "$$\n",
    "\n",
    "Тогда разность:\n",
    "$$\n",
    "I(i+u,j+v)-I(i,j)\\approx I_x(i,j),u + I_y(i,j),v\n",
    "$$\n",
    "\n",
    "Подставляем в $E(u,v)$:\n",
    "$$\n",
    "E(u,v)\\approx \\sum_{(i,j)\\in W} w(i,j),(I_x u + I_y v)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "(I_x u + I_y v)^2 = I_x^2 u^2 + 2 I_x I_y u v + I_y^2 v^2\n",
    "$$\n",
    "\n",
    "Тогда:\n",
    "$$\n",
    "E(u,v)\\approx u^2\\sum w I_x^2+2uv\\sum w I_x I_y+v^2\\sum w I_y^2\n",
    "$$\n",
    "\n",
    "В матричном виде:\n",
    "$$\n",
    "E(u,v)\\approx\n",
    "\\begin{bmatrix}u & v\\end{bmatrix}\\mathbf{M}\\begin{bmatrix}u \\\\ v\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "где $\\mathbf{M}$ — **матрица вторых моментов**:\n",
    "$$\n",
    "\\mathbf{M}=\\sum_{(i,j)\\in W} w(i,j)\\begin{bmatrix}I_x^2 & I_x I_y \\\\ I_x I_y & I_y^2 \\end{bmatrix} = \\begin{bmatrix}A & C \\\\ C & B\\end{bmatrix}\n",
    "$$\n",
    "с обозначениями:\n",
    "$$\n",
    "A=\\sum w I_x^2,\\quad\n",
    "B=\\sum w I_y^2,\\quad\n",
    "C=\\sum w I_x I_y\n",
    "$$\n",
    "\n",
    "#### Собственные значения и геометрический смысл\n",
    "\n",
    "Пусть $\\lambda_1,\\lambda_2$ — собственные значения $\\mathbf{M}$ (они $\\ge 0$)\n",
    "\n",
    "- **Ровная область**: градиенты малы $A,B,C\\approx 0 \\rightarrow \\lambda_1\\approx 0, \\lambda_2\\approx 0$\n",
    "- **Ребро**: сильное изменение в одном направлении – одно собственное значение большое, второе маленькое $\\lambda_1\\gg \\lambda_2\\approx 0$\n",
    "- **Угол**: изменения в двух направлениях – $\\lambda_1 > 0, \\lambda_2 > 0$\n",
    "\n",
    "#### Скалярный отклик\n",
    "\n",
    "$$\n",
    "R = \\det(\\mathbf{M}) - k,(\\operatorname{trace}(\\mathbf{M}))^2\n",
    "$$\n",
    "где\n",
    "$$\n",
    "\\det(\\mathbf{M}) = \\lambda_1\\lambda_2,\\quad\n",
    "\\operatorname{trace}(\\mathbf{M}) = \\lambda_1+\\lambda_2\n",
    "$$\n",
    "и обычно $k\\in[0.04, 0.06]$\n",
    "\n",
    "Если $\\mathbf{M}=\\begin{bmatrix}A & C \\\\ C & B\\end{bmatrix}$, то:\n",
    "\n",
    "$$\n",
    "\\det(\\mathbf{M}) = AB - C^2\n",
    "$$\n",
    "$$\n",
    "\\operatorname{trace}(\\mathbf{M}) = A + B\n",
    "$$\n",
    "\n",
    "Значит:\n",
    "$$\n",
    "R = (AB - C^2) - k(A+B)^2\n",
    "$$\n",
    "\n",
    "Интерпретация:\n",
    "\n",
    "- **Ровная область**: $R\\approx 0$\n",
    "- **Ребро**: одно значение большое, другое маленькое $\\rightarrow \\det \\approx 0, \\operatorname{trace} > 0 \\rightarrow R$ часто **отрицательный**\n",
    "- **Угол**: $A$ и $B$ большие, $\\det > 0 \\rightarrow R$ обычно **сильно положительный**\n",
    "\n",
    "\n",
    "#### Практический алгоритм\n",
    "\n",
    "1. Перевести в `grayscale`\n",
    "2. Посчитать градиенты $I_x, I_y$ (обычно Sobel)\n",
    "3. Посчитать $I_x^2, I_y^2, I_x I_y$\n",
    "4. Сгладить их гауссовым фильтром\n",
    "5. Для каждой точки вычислить $R$\n",
    "6. Взять точки, где $R$ больше порога"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4efd63-fab9-4087-8c41-dd38a0ff0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "import warnings\n",
    "import glob\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def show_image(image, cmap=None):\n",
    "    plt.imshow(image, cmap=cmap)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "SUDOKU_IMAGE_PATH = \"assets/lesson_2_example_image_1.jpg\"\n",
    "OUTDOOR_FIRST_PART_PATH = \"assets/lesson_2_example_image_2.jpg\"\n",
    "OUTDOOR_SECOND_PART_PATH = \"assets/lesson_2_example_image_3.jpg\"\n",
    "OUTDOOR_THIRD_PART_PATH = \"assets/lesson_2_example_image_4.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7891343-1c47-4bea-8eea-6e1d2bb5b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harris_example(img, nms: bool = False) -> None:\n",
    "    title = f\"Harris Corner Detection (nms is {'on' if nms else 'off'})\"\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray_f = np.float32(gray)\n",
    "\n",
    "    block_size = 2\n",
    "    ksize = 3\n",
    "    k = 0.04\n",
    "\n",
    "    R = cv2.cornerHarris(gray_f, blockSize=block_size, ksize=ksize, k=k)\n",
    "    if not nms:\n",
    "        R = cv2.dilate(R, None)\n",
    "    thr = 0.01 * R.max()\n",
    "    mask_thr = (R > thr)\n",
    "\n",
    "    if nms:\n",
    "        nms_ksize = 7\n",
    "        kernel = np.ones((nms_ksize, nms_ksize), np.uint8)\n",
    "        R_dilated = cv2.dilate(R, kernel)\n",
    "        mask_localmax = (R == R_dilated)\n",
    "\n",
    "        corners_mask = mask_thr & mask_localmax\n",
    "\n",
    "        ys, xs = np.where(corners_mask)\n",
    "\n",
    "        result = img.copy()\n",
    "        for x, y in zip(xs, ys):\n",
    "            cv2.circle(result, (int(x), int(y)), 3, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "    else:\n",
    "\n",
    "        result = img.copy()\n",
    "        result[mask_thr] = (0, 0, 255)\n",
    "\n",
    "    plt.title(title)\n",
    "    show_image(result[..., ::-1])\n",
    "\n",
    "img = cv2.imread(SUDOKU_IMAGE_PATH)\n",
    "harris_example(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae154aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# harris_example(cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c19c33b-9a06-49b8-b903-031598b07d5e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c838fd",
   "metadata": {},
   "source": [
    "*Напоминание: оператор Собеля*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f1a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori = cv2.cvtColor(cv2.imread(SUDOKU_IMAGE_PATH), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "gradientx = cv2.Sobel(ori, ddepth=cv2.CV_32F, dx=1, dy=0)\n",
    "gradienty = cv2.Sobel(ori, ddepth=cv2.CV_32F, dx=0, dy=1)\n",
    "gradient = np.concat((gradientx, gradienty), axis=1)\n",
    "\n",
    "show_image(np.abs(gradient), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cdda9a",
   "metadata": {},
   "source": [
    "#### Laplacian of Gaussian (LoG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a45148",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread(OUTDOOR_FIRST_PART_PATH)\n",
    "gray = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "blur = cv2.GaussianBlur(gray, (0, 0), sigmaX=1.6, sigmaY=1.5)\n",
    "\n",
    "# # Эквивалентно\n",
    "# logx = cv2.Sobel(blur, ddepth=cv2.CV_32F, dx=2, dy=0)\n",
    "# logy = cv2.Sobel(blur, ddepth=cv2.CV_32F, dx=0, dy=2)\n",
    "# log = logx + logy\n",
    "log = cv2.Laplacian(blur, ddepth=cv2.CV_32F, ksize=3)\n",
    "\n",
    "# Для визуализации\n",
    "log_abs = np.abs(log)\n",
    "log_vis = cv2.normalize(log_abs, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "plt.title(\"LoG\")\n",
    "show_image(log_vis, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd60668",
   "metadata": {},
   "source": [
    "#### Difference of Gaussian (DoG)\n",
    "\n",
    "\"Дешевое\" приближение `LoG`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e1e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread(OUTDOOR_FIRST_PART_PATH)\n",
    "gray = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "sigma1 = 1.6\n",
    "sigma2 = 1.6 * (2 ** 0.5)\n",
    "\n",
    "# ksize ~ 6*sigma + 1\n",
    "g1 = cv2.GaussianBlur(gray, ksize=(0, 0), sigmaX=sigma1, sigmaY=sigma1)\n",
    "g2 = cv2.GaussianBlur(gray, ksize=(0, 0), sigmaX=sigma2, sigmaY=sigma2) \n",
    "\n",
    "dog = g1.astype(np.float32) - g2.astype(np.float32)\n",
    "\n",
    "# Для визуализации\n",
    "dog_vis = cv2.normalize(dog, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "plt.title(\"DoG\")\n",
    "show_image(dog_vis, cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2df195",
   "metadata": {},
   "source": [
    "#### Gaussian Pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b636956",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = test_image.copy()\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "num_levels = 5\n",
    "pyramid = [img]\n",
    "\n",
    "current = img\n",
    "for _ in range(num_levels - 1):\n",
    "    # (5, 5) - размер ядра для сглаживания\n",
    "    current = cv2.pyrDown(current)\n",
    "    pyramid.append(current)\n",
    "\n",
    "\n",
    "# отрисовка пирамиды\n",
    "h0, w0 = pyramid[0].shape[:2]\n",
    "\n",
    "canvas_width = w0 + pyramid[1].shape[1]\n",
    "canvas_height = h0\n",
    "\n",
    "canvas = np.ones((canvas_height, canvas_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "canvas[0:h0, 0:w0] = pyramid[0]\n",
    "\n",
    "y_offset = 0\n",
    "for level in pyramid[1:]:\n",
    "    h, w = level.shape[:2]\n",
    "    canvas[y_offset:y_offset+h, w0:w0+w] = level\n",
    "    y_offset += h\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(canvas)\n",
    "plt.title(\"Gaussian Pyramid (real relative sizes)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba090f21",
   "metadata": {},
   "source": [
    "### SIFT\n",
    "\n",
    "*Scale-Invariant Feature Transform*\n",
    "\n",
    "Алгоритм для поиска признаков на изображениях, устойчивый к масштабированию и поворотам\n",
    "\n",
    "![](./assets/lesson_2_image_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32485f4e",
   "metadata": {},
   "source": [
    "Шаги:\n",
    "**1. Построение масштабного пространства (Scale-space construction)**\n",
    "\n",
    "- Исходное изображение $I(x,y)$ сглаживается:\n",
    "\n",
    "$$\n",
    "L(x,y,\\sigma) = G(x,y,\\sigma) * I(x,y)\n",
    "$$\n",
    "\n",
    "- Строится несколько уровней размытия (scale levels) для каждого масштаба\n",
    "- Масштабы группируются в **октавы**:\n",
    "  - при переходе к следующей октаве изображение уменьшается в 2 раза\n",
    "  - процесс повторяется\n",
    "\n",
    "**2. Вычисление Difference-of-Gaussians (DoG)**\n",
    "\n",
    "\n",
    "- Для соседних масштабов вычисляется разность:\n",
    "$$\n",
    "D(x,y,\\sigma) = L(x,y,k\\sigma) - L(x,y,\\sigma)\n",
    "$$\n",
    "- Получается пирамида DoG-изображений\n",
    "\n",
    "![](./assets/lesson_2_image_3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1045a49",
   "metadata": {},
   "source": [
    "**3. Поиск экстремумов в scale-space**\n",
    "\n",
    "Цель — найти потенциальные ключевые точки\n",
    "\n",
    "- Каждая точка DoG сравнивается:\n",
    "  - с 8 соседями в том же масштабе\n",
    "  - с 9 соседями в масштабе ниже\n",
    "  - с 9 соседями в масштабе выше\n",
    "\n",
    "- Если точка — локальный максимум или минимум, то кандидат в ключевую точку\n",
    "\n",
    "![](./assets/lesson_2_image_4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f8f0d",
   "metadata": {},
   "source": [
    "**4. Точная локализация ключевых точек**\n",
    "\n",
    "Фильтрация точек\n",
    "\n",
    "- Раскладывается ряд Тейлора для $D(x,y,\\sigma)$ и откидываются точки низкой интенсивности   (`contrastThreshold`)\n",
    "- Удаляются граничные точки, по аналогии с Harris detector (`edgeThreshold`)\n",
    "\n",
    "**5. Назначение ориентации (Orientation assignment)**\n",
    "\n",
    "Необходимо для rotation invariance\n",
    "\n",
    "- В окрестности ключевой точки вычисляются:\n",
    "  - величина градиента\n",
    "  - направление градиента\n",
    "- Строится гистограмма направлений\n",
    "- Основной пик гистограммы задаёт ориентацию точки\n",
    "\n",
    "**6. Построение дескриптора ключевой точки**\n",
    "\n",
    "- Окрестность точки:\n",
    "  - масштабируется\n",
    "  - поворачивается по найденной ориентации\n",
    "\n",
    "- Область делится на сетку $4 \\times 4$\n",
    "- В каждой ячейке строится гистограмма направлений градиента (8 направлений)\n",
    "- Формируется вектор размерности:\n",
    "$$\n",
    "4 \\times 4 \\times 8 = 128\n",
    "$$\n",
    "\n",
    "**7. Нормализация дескриптора**\n",
    "\n",
    "Цель — устойчивость к изменению освещения\n",
    "\n",
    "- Дескриптор нормируется по L2-норме\n",
    "- Значения ограничиваются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef5a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(OUTDOOR_FIRST_PART_PATH)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "sift = cv2.SIFT_create()\n",
    "kp = sift.detect(gray,None)\n",
    " \n",
    "# img = cv2.drawKeypoints(gray, kp, img)\n",
    "# # Отрисовка с направлениями и магнитудой\n",
    "img = cv2.drawKeypoints(gray, kp, img, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "show_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa4667",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119b26c",
   "metadata": {},
   "source": [
    "#### FAST\n",
    "\n",
    "![](./assets/lesson_2_image_5.jpg)\n",
    "\n",
    "[link](https://habr.com/ru/articles/414459/)\n",
    "\n",
    "FAST был одним из первых эвристических методов поиска особых точек, который завоевал большую популярность из-за своей вычислительной эффективности. Для принятия решения о том, считать заданную точку С особой или нет, в этом методе рассматривается яркость пикселей на окружности с центром в точке С и радиусом 3:\n",
    "Сравнивая яркости пикселей окружности с яркостью центра C, получаем для каждого три возможных исхода (светлее, темнее, похоже):\n",
    "\n",
    "$\\begin{array}{l} {I_p} > {I_C} + t\\\\ {I_p} < {I_C}-t\\\\ {I_C}-t < {I_p} < {I_C} + t\\end{array}$\n",
    "\n",
    "Здесь I – яркость пикселей, t – некоторый заранее фиксированный порог по яркости.\n",
    "Точка помечается как особая, если на круге существует подряд n=12 пикселей, которые темнее, или 12 пикселей, которые светлее, чем центр.\n",
    "\n",
    "Как показала практика, в среднем для принятия решения нужно было проверить около 9 точек. Для того, чтобы ускорить процесс, авторы предложили сначала проверить только четыре пиксела под номерами: 1, 5, 9, 13. Если среди них есть 3 пиксела светлее или темнее, то выполняется полная проверка по 16 точкам, иначе – точка сразу помечается как «не особая». Это сильно сокращает время работы, для принятия решения в среднем достаточно опросить всего лишь около 4 точек окружности.\n",
    "\n",
    "---\n",
    "\n",
    "#### BRIEF\n",
    "\n",
    "![](./assets/lesson_2_image_6.jpg)\n",
    "\n",
    "Алгоритм вычисления дескрипторов особых точек. строится из 256 бинарных сравнений между яркостями пикселей на размытом изображении. Бинарный тест между точками х и у определяется так:\n",
    "\n",
    "$$\n",
    "\\tau(P, x, y) := \n",
    "\\begin{cases}\n",
    "1 & \\text{if } p(x) < p(y), \\\\\n",
    "0 & \\text{if } p(x) \\geq p(y).\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "В оригинальной статье было рассмотрено несколько способов выбора точек для бинарных сравнений. Как оказалось, один из лучших способов – выбирать точки случайным образом Гауссовским распределением вокруг центрального пиксела. Эта случайная последовательность точек выбирается один раз и в дальнейшем не меняется. Размер рассматриваемой окрестности точки равен 31х31 пиксел, а апертура размытия равна 5.\n",
    "\n",
    "Полученный бинарный дескриптор оказывается устойчив к сменам освещения, перспективному искажению, быстро вычисляется и сравнивается, но очень неустойчив к вращению в плоскости изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d9108",
   "metadata": {},
   "source": [
    "### ORB \n",
    "\n",
    "(Oriented FAST and Rotated BRIEF)\n",
    "\n",
    "1. Детектор ключевых точек: FAST\n",
    "2. Оценка ключевых точек\n",
    "- считает Harris score для точки\n",
    "- оставляет топ-N точек\n",
    "3. Устойчивость к масштабу: пирамида изображений\n",
    "- строится пирамида: изображение в нескольких масштабах (уменьшенные копии)\n",
    "- FAST запускается на каждом уровне\n",
    "4. Ориентация ключевой точки (Oriented)\n",
    "- смотрим на распределение яркости вокруг точки\n",
    "- находим куда смещён центр массы яркости\n",
    "1. Дескриптор: Rotated BRIEF\n",
    "- ORB делает rBRIEF: набор пар точек для сравнений поворачивается согласно ориентации ключевой точки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfb306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(OUTDOOR_FIRST_PART_PATH, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "kp = orb.detect(img,None)\n",
    "kp, des = orb.compute(img, kp)\n",
    " \n",
    "img2 = cv2.drawKeypoints(cv2.imread(OUTDOOR_FIRST_PART_PATH), kp, None, color=(0,0,255), flags=0)\n",
    "show_image(img2[..., ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b848b2c4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb37d27",
   "metadata": {},
   "source": [
    "## Matching\n",
    "\n",
    "### Brute Force Matcher\n",
    "\n",
    "#### Ratio Test (Lowe)\n",
    "\n",
    "Соответствия ключевых точек между двумя изображениями находятся путём определения их ближайших соседей. Однако в некоторых случаях второе ближайшее совпадение может быть очень близко к первому. Это может происходить из-за шума или по другим причинам. В таком случае вычисляется отношение расстояния до ближайшего соседа к расстоянию до второго ближайшего. Если это отношение больше 0,8, совпадение отбрасывается. Согласно статье, такой подход устраняет около 90% ложных совпадений, теряя при этом лишь 5% правильных. ([source](https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread(OUTDOOR_FIRST_PART_PATH, cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread(OUTDOOR_SECOND_PART_PATH, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "# Матчинг дескрипторов\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "matches_knn = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "good = []\n",
    "ratio = 0.8\n",
    "for m, n in matches_knn:\n",
    "    if m.distance < ratio * n.distance:\n",
    "        good.append(m)\n",
    "\n",
    "\n",
    "good = sorted(good, key=lambda x: x.distance)\n",
    "out = cv2.drawMatches(\n",
    "    cv2.imread(OUTDOOR_FIRST_PART_PATH), kp1,\n",
    "    cv2.imread(OUTDOOR_SECOND_PART_PATH), kp2,\n",
    "    good[:100], None,\n",
    "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    ")\n",
    "\n",
    "show_image(out[..., ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39e821e-cc10-4cc3-9e37-a100d1d30ca6",
   "metadata": {},
   "source": [
    "### FLANN (Fast Library for Approximate Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110797be",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread(OUTDOOR_FIRST_PART_PATH, cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread(OUTDOOR_SECOND_PART_PATH, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "sift = cv2.ORB_create()\n",
    "kp1 = orb.detect(img1,None)\n",
    "kp2 = orb.detect(img2,None)\n",
    "\n",
    "kp1, des1 = sift.compute(img1, kp1)\n",
    "kp2, des2 = sift.compute(img2, kp2)\n",
    "\n",
    "# Матчинг дескрипторов\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1, des2)\n",
    "\n",
    "matches = sorted(matches, key=lambda m: m.distance)\n",
    "\n",
    "top_n = 50\n",
    "matched_vis = cv2.drawMatches(\n",
    "    cv2.imread(OUTDOOR_FIRST_PART_PATH), kp1,\n",
    "    cv2.imread(OUTDOOR_SECOND_PART_PATH), kp2,\n",
    "    matches[:top_n], None,\n",
    "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    ")\n",
    "\n",
    "show_image(matched_vis[..., ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b9b307",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c2bdad",
   "metadata": {},
   "source": [
    "### FLANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d588be",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread(OUTDOOR_FIRST_PART_PATH, cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread(OUTDOOR_SECOND_PART_PATH, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches_knn = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "good = []\n",
    "ratio = 0.75\n",
    "for m, n in matches_knn:\n",
    "    if m.distance < ratio * n.distance:\n",
    "        good.append(m)\n",
    "\n",
    "out = cv2.drawMatches(\n",
    "    cv2.imread(OUTDOOR_FIRST_PART_PATH), kp1,\n",
    "    cv2.imread(OUTDOOR_SECOND_PART_PATH), kp2,\n",
    "    good, None,\n",
    "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    ")\n",
    "\n",
    "show_image(out[..., ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f2f29-6193-4e03-97a5-9e3b9ebe07d9",
   "metadata": {},
   "source": [
    "### RANSAC\n",
    "\n",
    "Random Sample Consensus\n",
    "\n",
    "Алгоритм:\n",
    "\n",
    "- случайно выбрать минимальное число точек\n",
    "- построить модель\n",
    "- проверить, сколько точек согласуются с моделью\n",
    "- повторить много раз\n",
    "- выбрать модель с наибольшим числом согласованных точек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6ccb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# ============== код аналогичен клетке выше ====================\n",
    "# ==============================================================\n",
    "img1 = cv2.imread(OUTDOOR_FIRST_PART_PATH, cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread(OUTDOOR_SECOND_PART_PATH, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches_knn = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "good = []\n",
    "ratio = 0.75\n",
    "for m, n in matches_knn:\n",
    "    if m.distance < ratio * n.distance:\n",
    "        good.append(m)\n",
    "# ==============================================================\n",
    "\n",
    "# RANSAC\n",
    "src_pts = np.float32([kp1[m.queryIdx].pt for m in good])\n",
    "dst_pts = np.float32([kp2[m.trainIdx].pt for m in good])\n",
    "\n",
    "H, mask = cv2.findHomography(\n",
    "    src_pts,\n",
    "    dst_pts,\n",
    "    cv2.RANSAC,\n",
    "    5.0\n",
    ")\n",
    "inliers = [good[i] for i in range(len(good)) if mask[i]]\n",
    "\n",
    "print(\"Matches after ratio test:\", len(good))\n",
    "print(\"Inliers after RANSAC:\", len(inliers))\n",
    "\n",
    "img_matches = cv2.drawMatches(\n",
    "    cv2.imread(OUTDOOR_FIRST_PART_PATH), kp1,\n",
    "    cv2.imread(OUTDOOR_SECOND_PART_PATH), kp2,\n",
    "    inliers,\n",
    "    None,\n",
    "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    ")\n",
    "\n",
    "show_image(img_matches[..., ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feef2617-dfa1-4bd4-83b5-03f9dc8cda51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на найденную ransac гомографию\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 6))\n",
    "\n",
    "ax[0].imshow(cv2.warpPerspective(img1, H, (img1.shape[1], img2.shape[0])), cmap=\"gray\")\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(img2, cmap=\"gray\")\n",
    "ax[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16728221-eab8-4f46-8992-5128dd40f69a",
   "metadata": {},
   "source": [
    "## Классические детекторы\n",
    "\n",
    "### Viola-Jones (Haar Cascade)\n",
    "\n",
    "[Подробнее](https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html)\n",
    "\n",
    "\n",
    "**Идея**\n",
    "\n",
    "Это классический детектор объектов (часто — лиц), который:\n",
    "\n",
    "1. Сканирует изображение \"окном\" разных размеров (scale pyramid)\n",
    "2. В каждом окне быстро считает простые признаки (Haar-признаки)\n",
    "3. Прогоняет окно через каскад простых классификаторов и рано отбрасывает почти все окна как False\n",
    "4. Для оставшихся окон делает NMS пересекающихся детекций.\n",
    "\n",
    "**Haar-like features**\n",
    "\n",
    "![](assets/lesson_2_image_7.jpg)\n",
    "\n",
    "**Каскад и обучение**\n",
    "\n",
    "Каждый этап каскада - это сильный классификатор *AdaBoost*, собранный из слабых:\n",
    "\n",
    "- Слабый классификатор: если конкретный Haar-признак > порога, это лицо, иначе нет\n",
    "- AdaBoost выбирает наиболее полезные признаки и веса.\n",
    "\n",
    "\n",
    "Пример: [haar-cascade.py](./haar-cascade.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8421e75f-7ca6-4882-ab9b-1e86de196f4c",
   "metadata": {},
   "source": [
    "### HOG + SVM\n",
    "\n",
    "[Подробнее про HOG](https://learnopencv.com/histogram-of-oriented-gradients/)\n",
    "\n",
    "**Интуиция**\n",
    "\n",
    "- Вместо яркости — использовать градиенты (объект определяется формой)\n",
    "\n",
    "**Алгоритм**\n",
    "\n",
    "- считаем градиенты изображения\n",
    "- строим гистограммы направлений\n",
    "- нормализуем блоки\n",
    "- классифицируем окно через SVM\n",
    "\n",
    "Пример: [hog.py](hog.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623bbf44-abf6-4e57-a092-ac0f0f0eb2c8",
   "metadata": {},
   "source": [
    "## Camera Calibration\n",
    "\n",
    "Из-за несовершенства камер и их линз мы можем видеть всякого рода [distortions](https://en.wikipedia.org/wiki/Distortion_%28optics%29)\n",
    "\n",
    "![](./assets/lesson_2_image_8.jpg)\n",
    "\n",
    "Два основных вида искажений – radial & tangential\n",
    "\n",
    "*Radial distortion*\n",
    "$$\n",
    "\\begin{matrix}\n",
    "x_{distorted} = x( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6) \\\\\n",
    "y_{distorted} = y( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "*tangential distortion*\n",
    "$$\n",
    "\\begin{matrix}\n",
    "x_{distorted} = x + [ 2p_1xy + p_2(r^2+2x^2)] \\\\\n",
    "y_{distorted} = y + [ p_1(r^2+ 2y^2)+ 2p_2xy]\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Для их исправления искажений необходимо найти\n",
    "\n",
    "$$\n",
    "Distortion \\; coefficients=(k_1 \\hspace{10pt} k_2 \\hspace{10pt} p_1 \\hspace{10pt} p_2 \\hspace{10pt} k_3)\n",
    "$$\n",
    "\n",
    "В дополнение к этому, необходимо найти *Intrinsic (внутренние)* и *extrinsic (внешние)* параметры камеры\n",
    "\n",
    "*Intrinsic* параметры включают в себя *focal length (фокусное расстояние)* и *optical centers (оптический центр)*. Фокусное расстояние и оптические центры могут быть использованы для построения матрицы камеры, которая применяется для устранения искажений, вызванных линзами конкретной камеры. Матрица камеры является уникальной для данной камеры, поэтому после вычисления её можно повторно использовать для других изображений, снятых той же камерой. Она выражается в виде матрицы размером 3×3.\n",
    "\n",
    "$$\n",
    "camera \\; matrix = \\left [ \\begin{matrix}   f_x & 0 & c_x \\\\  0 & f_y & c_y \\\\   0 & 0 & 1 \\end{matrix} \\right ]\n",
    "$$\n",
    "\n",
    "Внешние параметры (extrinsic parameters) соответствуют векторам вращения и переноса, которые переводят координаты 3D-точки в систему координат камеры.\n",
    "\n",
    "$$\n",
    "x\\approx K[R∣t]X\n",
    "$$\n",
    "\n",
    "где $x$ - пиксель, $X$ - координаты точки 3d, $K – camera \\; matrix$, $R$ - матрица поворота, $t$ - вектор сдвига"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447487b0-3c69-4f0e-ab72-46ad600f53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка, скачивание файлов\n",
    "os.makedirs(\"chessboard\", exist_ok=True)\n",
    "\n",
    "for i in range(1, 15):\n",
    "    if os.path.exists(f\"chessboard/left{str(i).zfill(2)}.jpg\"):\n",
    "        continue\n",
    "    # there is no left10.jpg image\n",
    "    if i == 10:\n",
    "        continue\n",
    "    url = f\"https://raw.githubusercontent.com/opencv/opencv/refs/heads/master/samples/data/left{str(i).zfill(2)}.jpg\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        response = requests.get(url, verify=False)\n",
    "    with open(f\"chessboard/left{str(i).zfill(2)}.jpg\", \"wb\") as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f2c0d4-dba5-46a2-9e85-8b6ed410291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = sorted(glob.glob(\"chessboard/*.jpg\"))\n",
    "assert len(img_paths) > 0, \"Не нашел изображений в chessboard/*.jpg\"\n",
    "\n",
    "show_image(cv2.imread(img_paths[10])[..., ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85374f39-294d-41fe-893c-1965413cfcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_size = (9, 6)\n",
    "\n",
    "objp = np.zeros((pattern_size[0] * pattern_size[1], 3), np.float32)\n",
    "objp[:, :2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1, 2)\n",
    "\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 1e-3)\n",
    "\n",
    "objpoints = []\n",
    "imgpoints = []\n",
    "good = []\n",
    "\n",
    "img_shape = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae8096-c2e7-4999-9340-da26da22d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ищем подходящие кадры и координаты углов клеток\n",
    "for p in img_paths:\n",
    "    img = cv2.imread(p)\n",
    "    if img is None:\n",
    "        continue\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_shape = gray.shape[::-1]\n",
    "\n",
    "    found, corners = cv2.findChessboardCorners(\n",
    "        gray, pattern_size,\n",
    "        flags=cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_NORMALIZE_IMAGE\n",
    "    )\n",
    "\n",
    "    if not found:\n",
    "        print(\"Не найдено углов:\", Path(p).name)\n",
    "        continue\n",
    "\n",
    "    corners2 = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)\n",
    "\n",
    "    objpoints.append(objp)\n",
    "    imgpoints.append(corners2)\n",
    "    good.append(p)\n",
    "\n",
    "print(\"Подходящих изображений:\", len(good))\n",
    "assert len(good) >= 5, \"Слишком мало удачных кадров — попробуй больше изображений или проверь pattern_size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6cf1c1-472f-43fc-92e5-73b2c02c3e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = good[0]\n",
    "img = cv2.imread(sample_path)\n",
    "vis = img.copy()\n",
    "plt.title(f\"Углы: {Path(sample_path).name}\")\n",
    "show_image(cv2.drawChessboardCorners(vis, pattern_size, imgpoints[0], True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a45fd-07bb-4a15-9296-39b6f04642f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# достаем параметры камеры\n",
    "ret, K, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_shape, None, None)\n",
    "\n",
    "print(\"RMS (ret):\", ret)\n",
    "print(\"K:\\n\", K)\n",
    "print(\"dist:\", dist.ravel())\n",
    "print(\"Views:\", len(rvecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d031c-b885-4b05-8d11-b926fd56e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = good[0]\n",
    "img = cv2.imread(test_path)\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "# https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html\n",
    "# If the scaling parameter alpha=0, it returns undistorted image with minimum unwanted pixels.\n",
    "# So it may even remove some pixels at image corners.\n",
    "# If alpha=1, all pixels are retained with some extra black images\n",
    "newK, roi = cv2.getOptimalNewCameraMatrix(K, dist, (w, h), alpha=1.0)\n",
    "undist = cv2.undistort(img, K, dist, None, newK)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Оригинал\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2.cvtColor(undist, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"cv2.undistort\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c27bb9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0598f2a1-2775-4e7d-a252-432db385411f",
   "metadata": {},
   "source": [
    "### PnP\n",
    "\n",
    "**Perspective-n-Point**\n",
    "\n",
    "\n",
    "*Аналогия*\n",
    "- на столе лежит шахматка\n",
    "- ты знаешь координаты её углов\n",
    "- кто-то сделал фотографию\n",
    "- тебе дали только фото\n",
    "\n",
    "Вопрос: откуда была сделана фотография?\n",
    "\n",
    "То есть нужно найти:\n",
    "\n",
    "- позицию камеры\n",
    "- ориентацию камеры\n",
    "\n",
    "так, чтобы проекция 3D-точек совпала с наблюдаемыми пикселями.\n",
    "\n",
    "\n",
    "*Алгоритм*\n",
    "- Берёт предположение о положении камеры\n",
    "- Проецирует 3D точки в изображение\n",
    "- Сравнивает с реальными пикселями\n",
    "- Двигает камеру так, чтобы ошибка уменьшалась\n",
    "\n",
    "То есть минимизируется:\n",
    "\n",
    "$$\n",
    "\\sum_i ||x_i - \\hat{x}_i(R,t)||^2\n",
    "$$\n",
    "\n",
    "где:\n",
    "\n",
    "- $x_i$ – наблюдаемая точка\n",
    "- $\\hat{x}_i$ – спроецированная точка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7260793e-dcd9-40ae-bdf8-099bcc4a62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = 3\n",
    "test_path = good[indx]\n",
    "img = cv2.imread(test_path)\n",
    "\n",
    "ok, rvec, tvec = cv2.solvePnP(objp, imgpoints[indx], K, dist)\n",
    "\n",
    "vis = img.copy()\n",
    "cv2.drawFrameAxes(vis, K, dist, rvec, tvec, length=3)\n",
    "\n",
    "plt.title(\"solvePnP\")\n",
    "show_image(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc32c86-f396-4b1a-a60e-4fd9ffed7300",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be2797c-6a50-4d17-83f4-d4003efd123c",
   "metadata": {},
   "source": [
    "### Kalman Filter\n",
    "\n",
    "[Википедия](https://ru.wikipedia.org/wiki/%D0%A4%D0%B8%D0%BB%D1%8C%D1%82%D1%80_%D0%9A%D0%B0%D0%BB%D0%BC%D0%B0%D0%BD%D0%B0)\n",
    "\n",
    "![](./assets/lesson_2_image_9.png)\n",
    "\n",
    "Разберем только 1D пример. В размерностях больше принцип тот же, но математических выкладок больше"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433ea0a-3719-47f3-a457-52e39e4b6c5d",
   "metadata": {},
   "source": [
    "Фильтр Калмана — это алгоритм оценки истинного состояния системы по шумным измерениям, использующий:\n",
    "\n",
    "- модель динамики системы,\n",
    "- статистические свойства ошибок.\n",
    "\n",
    "**Постановка задачи**\n",
    "\n",
    "* $x_k$ - истинное состояние системы\n",
    "* $z_k$ - измерение\n",
    "* $u_k$ - управляющее воздействие\n",
    "* $\\psi_k$ - ошибка модели\n",
    "* $\\eta_k$ - ошибка измерения\n",
    "\n",
    "$$\n",
    "x_{k+1} = f(x_k, u_k) + \\psi_k\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_k = x_k + \\eta_k\n",
    "$$\n",
    "\n",
    "Предположения:\n",
    "\n",
    "1. Ошибки имеют нулевое среднее:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\psi_k] = 0, \\qquad \\mathbb{E}[\\eta_k] = 0\n",
    "$$\n",
    "\n",
    "2. Известны их дисперсии:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(\\psi_k), \\quad \\mathrm{Var}(\\eta_k)\n",
    "$$\n",
    "\n",
    "3. Ошибки независимы между собой и по времени\n",
    "\n",
    "\n",
    "**Алгоритм**\n",
    "\n",
    "Есть:\n",
    "\n",
    "- предсказание модели $\\hat{x}_{k|k-1}$\n",
    "- измерение $z_k$\n",
    "\n",
    "Оценка строится как линейная комбинация:\n",
    "\n",
    "$$\n",
    "\\hat{x}_k =\n",
    "(1-K_k)\\hat{x}_{k|k-1} + K_k z_k\n",
    "$$\n",
    "\n",
    "$K_k$ — **коэффициент Калмана**.\n",
    "\n",
    "**Вывод коэффициента Калмана**\n",
    "\n",
    "Ошибка оценки:\n",
    "\n",
    "$$\n",
    "e_k = \\hat{x}_k - x_k\n",
    "$$\n",
    "\n",
    "Хотим минимизировать квадрат ошибки\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[e_k^2] \\rightarrow \\min\n",
    "$$\n",
    "\n",
    "Минимизация приводит к:\n",
    "\n",
    "$$\n",
    "K_k =\n",
    "\\frac{P_{k|k-1}}\n",
    "{P_{k|k-1} + R}\n",
    "$$\n",
    "\n",
    "где:\n",
    "\n",
    "- $P_{k|k-1}$ дисперсия ошибки предсказания\n",
    "- $R$ дисперсия ошибки измерения\n",
    "\n",
    "\n",
    "После коррекции:\n",
    "\n",
    "$$\n",
    "P_k = (1 - K_k) P_{k|k-1}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Предсказание\n",
    "\n",
    "$$\n",
    "\\hat{x}_{k|k-1} = f(\\hat{x}_{k-1}, u_k)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{k|k-1} = P_{k-1} + Q\n",
    "$$\n",
    "\n",
    "где $Q$ дисперсия ошибки модели.\n",
    "\n",
    "\n",
    "#### Коррекция\n",
    "\n",
    "$$\n",
    "K_k = \\frac{P_{k|k-1}}{P_{k|k-1} + R}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{x}_k= \\hat{x}_{k|k-1}\n",
    " + K_k(z_k - \\hat{x}_{k|k-1})$$\n",
    "\n",
    "\n",
    "$$\n",
    "P_k = (1-K_k)P_{k|k-1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a8d63c-d4e5-4050-8ebb-9081c9d47188",
   "metadata": {},
   "source": [
    "Пример: [kalman.py](./kalman.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8246594a-6e8e-48df-a9b5-7fb36c2a0f24",
   "metadata": {},
   "source": [
    "### Panorama stitching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a71b12-b4f7-4ad0-9dd4-df509c4c028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_PLOTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658d193-d43a-428e-ac4a-e9c2481192a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def nonzero_mask(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    mask = (gray > 0).astype(np.uint8) * 255\n",
    "    return mask\n",
    "\n",
    "def crop_to_content(img):\n",
    "    mask = nonzero_mask(img)\n",
    "    ys, xs = np.where(mask > 0)\n",
    "    if len(xs) == 0 or len(ys) == 0:\n",
    "        return img\n",
    "    x0, x1 = xs.min(), xs.max()\n",
    "    y0, y1 = ys.min(), ys.max()\n",
    "    return img[y0:y1+1, x0:x1+1]\n",
    "\n",
    "# detection && description\n",
    "def create_feature(name):\n",
    "    if name == \"SIFT\":\n",
    "        return cv2.SIFT_create(nfeatures=4000)\n",
    "    else:\n",
    "        return cv2.ORB_create(nfeatures=6000)\n",
    "\n",
    "def detect_and_describe(feature, img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    kps, desc = feature.detectAndCompute(gray, None)\n",
    "    return kps, desc\n",
    "\n",
    "# FLANN\n",
    "def create_flann(kind):\n",
    "    if kind == \"SIFT\":\n",
    "        index_params = dict(algorithm=1, trees=5)   # 1 = FLANN_INDEX_KDTREE\n",
    "        search_params = dict(checks=50)\n",
    "    else:\n",
    "        index_params = dict(\n",
    "            algorithm=6,      # 6 = FLANN_INDEX_LSH\n",
    "            table_number=12,\n",
    "            key_size=20,\n",
    "            multi_probe_level=2\n",
    "        )\n",
    "        search_params = dict(checks=50)\n",
    "\n",
    "    return cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "# RANSAC\n",
    "def homography_ransac(kpsA, kpsB, matches, reproj_thresh=4.0):\n",
    "    if len(matches) < 4:\n",
    "        return None, None\n",
    "\n",
    "    ptsA = np.float32([kpsA[m.queryIdx].pt for m in matches])\n",
    "    ptsB = np.float32([kpsB[m.trainIdx].pt for m in matches])\n",
    "\n",
    "    H, mask = cv2.findHomography(ptsB, ptsA, cv2.RANSAC, reproj_thresh)\n",
    "    return H, mask  # mask: 1=inlier, 0=outlier\n",
    "\n",
    "def match_flann(flann, kind, descA, descB, ratio=0.75):\n",
    "    \"\"\"\n",
    "    KNN matching + ratio test (Lowe).\n",
    "    \"\"\"\n",
    "    if descA is None or descB is None:\n",
    "        return []\n",
    "\n",
    "    if kind == \"SIFT\":\n",
    "        descA = descA.astype(np.float32)\n",
    "        descB = descB.astype(np.float32)\n",
    "\n",
    "    knn = flann.knnMatch(descA, descB, k=2)\n",
    "\n",
    "    good = []\n",
    "    for m, n in knn:\n",
    "        if m.distance < ratio * n.distance:\n",
    "            good.append(m)\n",
    "    return good\n",
    "\n",
    "# составление композиции 2х изображений\n",
    "def estimate_canvas(imgA, imgB, H):\n",
    "    \"\"\"\n",
    "    Оцениваем размер холста для imgA + warped(imgB)\n",
    "    Возвращаем (canvas_w, canvas_h), offset=(ox, oy)\n",
    "    \"\"\"\n",
    "    hA, wA = imgA.shape[:2]\n",
    "    hB, wB = imgB.shape[:2]\n",
    "\n",
    "    cornersB = np.float32([[0, 0], [wB, 0], [wB, hB], [0, hB]]).reshape(-1, 1, 2)\n",
    "    warpedCornersB = cv2.perspectiveTransform(cornersB, H)\n",
    "\n",
    "    cornersA = np.float32([[0, 0], [wA, 0], [wA, hA], [0, hA]]).reshape(-1, 1, 2)\n",
    "\n",
    "    all_corners = np.vstack((cornersA, warpedCornersB)).reshape(-1, 2)\n",
    "\n",
    "    min_xy = all_corners.min(axis=0)\n",
    "    max_xy = all_corners.max(axis=0)\n",
    "\n",
    "    min_x, min_y = min_xy\n",
    "    max_x, max_y = max_xy\n",
    "\n",
    "    ox = int(np.floor(-min_x)) if min_x < 0 else 0\n",
    "    oy = int(np.floor(-min_y)) if min_y < 0 else 0\n",
    "\n",
    "    canvas_w = int(np.ceil(max_x - min_x))\n",
    "    canvas_h = int(np.ceil(max_y - min_y))\n",
    "\n",
    "    return (canvas_w, canvas_h), (ox, oy)\n",
    "\n",
    "\n",
    "def warp_to_canvas(img, H, canvas_size, offset):\n",
    "    \"\"\"Варпим img на общий холст, добавляя сдвиг offset\"\"\"\n",
    "    ox, oy = offset\n",
    "    T = np.array([[1, 0, ox],\n",
    "                  [0, 1, oy],\n",
    "                  [0, 0, 1]], dtype=np.float64)\n",
    "    Ht = T @ H\n",
    "    warped = cv2.warpPerspective(img, Ht, canvas_size)\n",
    "    return warped\n",
    "\n",
    "\n",
    "def compose_canvas(img_ref, canvas, offset):\n",
    "    \"\"\"\n",
    "    Кладём reference-изображение (img_ref) на уже созданный canvas\n",
    "    \"\"\"\n",
    "    ox, oy = offset\n",
    "    h, w = img_ref.shape[:2]\n",
    "    out = canvas.copy()\n",
    "    out[oy:oy+h, ox:ox+w] = img_ref\n",
    "    return out\n",
    "\n",
    "def simple_overlay(base, overlay):\n",
    "    \"\"\"\n",
    "    Простейшее смешивание\n",
    "    - Берём все непустые пиксели overlay и кладём их на base\n",
    "    \"\"\"\n",
    "    out = base.copy()\n",
    "    m = nonzero_mask(overlay)\n",
    "    out[m > 0] = overlay[m > 0]\n",
    "    return out\n",
    "\n",
    "# функции для отрисовки\n",
    "def draw_matches(imgA, kpsA, imgB, kpsB, matches, inlier_mask=None, title=\"matches\"):\n",
    "    \"\"\"\n",
    "    Рисуем matches\n",
    "    - Если inlier_mask задан, рисуем только inliers.\n",
    "    \"\"\"\n",
    "    if not DEBUG_PLOTS:\n",
    "        return\n",
    "\n",
    "    if inlier_mask is not None:\n",
    "        inlier_mask = inlier_mask.ravel().astype(bool)\n",
    "        matches = [m for m, ok in zip(matches, inlier_mask) if ok]\n",
    "\n",
    "    vis = cv2.drawMatches(\n",
    "        imgA, kpsA,\n",
    "        imgB, kpsB,\n",
    "        matches,\n",
    "        None,\n",
    "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "    )\n",
    "    plt.title(title)\n",
    "    show_image(vis[..., ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f681e7b-9b2b-44ae-b878-81933fdcbd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_two_simple_overlay(imgA, imgB, feature, kind):\n",
    "    \"\"\"\n",
    "    Склеивает imgB к imgA:\n",
    "    1) keypoints/desc\n",
    "    2) match FLANN\n",
    "    3) RANSAC homography (B -> A)\n",
    "    4) warp B на холст + кладём A\n",
    "    5) blending\n",
    "    \"\"\"\n",
    "    kpsA, descA = detect_and_describe(feature, imgA)\n",
    "    kpsB, descB = detect_and_describe(feature, imgB)\n",
    "\n",
    "    if DEBUG_PLOTS:\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        ax[0].imshow(cv2.drawKeypoints(imgA, kpsA, None)[..., ::-1])\n",
    "        ax[1].imshow(cv2.drawKeypoints(imgB, kpsB, None)[..., ::-1])\n",
    "        ax[0].axis('off')\n",
    "        ax[1].axis('off')\n",
    "        plt.title(f\"Дескрипторы ключевых точек с помощью {kind}\")\n",
    "        plt.show()\n",
    "\n",
    "    flann = create_flann(kind)\n",
    "    matches = match_flann(flann, kind, descA, descB, ratio=0.75)\n",
    "\n",
    "    # matches до RANSAC\n",
    "    draw_matches(imgA, kpsA, imgB, kpsB, matches, None, \"Matches (ratio test)\")\n",
    "\n",
    "    H, inlier_mask = homography_ransac(kpsA, kpsB, matches, reproj_thresh=4.0)\n",
    "    if H is None or inlier_mask is None:\n",
    "        raise RuntimeError(\"Not enough good matches / homography failed.\")\n",
    "\n",
    "    # matches после RANSAC (inliers)\n",
    "    draw_matches(imgA, kpsA, imgB, kpsB, matches, inlier_mask, \"Matches (RANSAC inliers)\")\n",
    "\n",
    "    canvas_size, offset = estimate_canvas(imgA, imgB, H)\n",
    "\n",
    "    warpedB = warp_to_canvas(imgB, H, canvas_size, offset)\n",
    "    if DEBUG_PLOTS:\n",
    "        plt.title(\"Warped B on canvas\")\n",
    "        show_image(warpedB[..., ::-1])\n",
    "\n",
    "    base = compose_canvas(imgA, warpedB, offset)\n",
    "    if DEBUG_PLOTS:\n",
    "        plt.title(\"Base (A placed onto canvas)\")\n",
    "        show_image(base[..., ::-1])\n",
    "\n",
    "    blended = simple_overlay(base, warpedB)\n",
    "    if DEBUG_PLOTS:\n",
    "        plt.title(\"Blended\")\n",
    "        show_image(blended[..., ::-1])\n",
    "\n",
    "    return blended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e437e8e-f564-4375-86c1-5f791df3bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread(OUTDOOR_FIRST_PART_PATH)\n",
    "img2 = cv2.imread(OUTDOOR_SECOND_PART_PATH)\n",
    "img3 = cv2.imread(OUTDOOR_THIRD_PART_PATH)\n",
    "\n",
    "kind = \"SIFT\"\n",
    "feature = create_feature(kind)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "\n",
    "ax[0].imshow(img1[..., ::-1])\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(img2[..., ::-1])\n",
    "ax[1].axis(\"off\")\n",
    "ax[2].imshow(img3[..., ::-1])\n",
    "ax[2].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6797704-7ff2-4604-9bf7-6095c2ddf807",
   "metadata": {},
   "outputs": [],
   "source": [
    "pano12 = stitch_two_simple_overlay(img1, img2, feature, kind)\n",
    "pano12 = crop_to_content(pano12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22941d0b-ef99-4e1c-8077-16f6ecbc3f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "pano123 = stitch_two_simple_overlay(pano12, img3, feature, kind)\n",
    "pano123 = crop_to_content(pano123)\n",
    "pano123_simple = pano123.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e796849-af90-4c88-a9fe-5c5a9d87064a",
   "metadata": {},
   "source": [
    "#### Poisson blending\n",
    "\n",
    "[блогпост](https://learnopencv.com/seamless-cloning-using-opencv-python-cpp/)\n",
    "\n",
    "[документация](https://docs.opencv.org/4.x/df/da0/group__photo__clone.html)\n",
    "\n",
    "Интуитивно - это способ вставить одну область изображения в другую так, чтобы граница вставки стала незаметной, даже если освещение или цвет отличаются\n",
    "\n",
    "\n",
    "**Алгоритм**\n",
    "\n",
    "1. Дано:\n",
    "- изображение-источник (что нужно вставить)\n",
    "- изображение-фон (куда вставляется)\n",
    "- маска - область, где должна происходить вставка\n",
    "\n",
    "2. Из источника берётся не цвет, а изменения цвета\n",
    "\n",
    "- где изображение становится светлее, где темнее\n",
    "- где есть границы объектов и текстуры\n",
    "- берется структура изменений а не сами значения цвета.\n",
    "- сохраняются детали, контуры и текстура\n",
    "\n",
    "3. Граница фиксируется по фону\n",
    "\n",
    "- По краю области вставки значения берутся из фонового изображения\n",
    "- граница точно совпадает с фоном\n",
    "- не возникает резкого скачка цвета на шве.\n",
    "\n",
    "4. Внутри области пересчитываются пиксели\n",
    "- Теперь алгоритм заполняет всю область вставки заново\n",
    "- старается сохранить изменения яркости как в источнике\n",
    "- но одновременно плавно подстроиться под значения на границе\n",
    "\n",
    "Если формально, то мы считаем градиент источника и решаем систему линейных уравнений, соответствующих [дискретному уравнению Пуассона](https://ru.wikipedia.org/wiki/%D0%A3%D1%80%D0%B0%D0%B2%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5_%D0%9F%D1%83%D0%B0%D1%81%D1%81%D0%BE%D0%BD%D0%B0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e1e53-0722-470f-a2b6-9153bae91f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_blend(base, overlay):\n",
    "    # 1) Строим маску overlay:\n",
    "    # где overlay НЕ чёрный (там есть реальные пиксели после warp)\n",
    "    # там mask=255, иначе 0\n",
    "    mask = nonzero_mask(overlay)\n",
    "\n",
    "    # 2) Если overlay пустой (вдруг он целиком чёрный) — нечего смешивать.\n",
    "    ys, xs = np.where(mask > 0)\n",
    "    if len(xs) == 0 or len(ys) == 0:\n",
    "        return base\n",
    "\n",
    "    # 3) Находим bounding box области, которую будем \"вшивать\"\n",
    "    # Это нужно, чтобы определить центр вставки\n",
    "    x0, x1 = xs.min(), xs.max()\n",
    "    y0, y1 = ys.min(), ys.max()\n",
    "\n",
    "    # 4) Центр вставки. seamlessClone просит \"куда вставлять\" в координатах base\n",
    "    # Мы берём центр bbox маски — обычно этого достаточно\n",
    "    center = (int((x0 + x1) / 2), int((y0 + y1) / 2))\n",
    "\n",
    "    # seamlessClone(src, dst, mask, center, mode)\n",
    "    #\n",
    "    # src = overlay (то, что вставляем)\n",
    "    # dst = base (фон)\n",
    "    # mask = область src, которую вставляем\n",
    "    # center = точка в dst, вокруг которой будет размещён src\n",
    "    # cv2.NORMAL_CLONE - классический Poisson blending\n",
    "    blended = cv2.seamlessClone(\n",
    "        overlay, base, mask, center, cv2.NORMAL_CLONE,\n",
    "    )\n",
    "\n",
    "    return blended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51583f17-15a7-45c8-98bc-be5c9b9871fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_two_poisson(imgA, imgB, feature, kind):\n",
    "    \"\"\"\n",
    "    Склеивает imgB к imgA:\n",
    "    1) keypoints/desc\n",
    "    2) match FLANN\n",
    "    3) RANSAC homography (B -> A)\n",
    "    4) warp B на холст + кладём A\n",
    "    5) blending\n",
    "    \"\"\"\n",
    "    kpsA, descA = detect_and_describe(feature, imgA)\n",
    "    kpsB, descB = detect_and_describe(feature, imgB)\n",
    "\n",
    "    if DEBUG_PLOTS:\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        ax[0].imshow(cv2.drawKeypoints(imgA, kpsA, None)[..., ::-1])\n",
    "        ax[1].imshow(cv2.drawKeypoints(imgB, kpsB, None)[..., ::-1])\n",
    "        ax[0].axis('off')\n",
    "        ax[1].axis('off')\n",
    "        plt.title(f\"Дескрипторы ключевых точек с помощью {kind}\")\n",
    "        plt.show()\n",
    "\n",
    "    flann = create_flann(kind)\n",
    "    matches = match_flann(flann, kind, descA, descB, ratio=0.75)\n",
    "\n",
    "    # matches до RANSAC\n",
    "    draw_matches(imgA, kpsA, imgB, kpsB, matches, None, \"Matches (ratio test)\")\n",
    "\n",
    "    H, inlier_mask = homography_ransac(kpsA, kpsB, matches, reproj_thresh=4.0)\n",
    "    if H is None or inlier_mask is None:\n",
    "        raise RuntimeError(\"Not enough good matches / homography failed.\")\n",
    "\n",
    "    # matches после RANSAC (inliers)\n",
    "    draw_matches(imgA, kpsA, imgB, kpsB, matches, inlier_mask, \"Matches (RANSAC inliers)\")\n",
    "\n",
    "    canvas_size, offset = estimate_canvas(imgA, imgB, H)\n",
    "\n",
    "    warpedB = warp_to_canvas(imgB, H, canvas_size, offset)\n",
    "    if DEBUG_PLOTS:\n",
    "        plt.title(\"Warped B on canvas\")\n",
    "        show_image(warpedB[..., ::-1])\n",
    "\n",
    "    base = compose_canvas(imgA, warpedB, offset)\n",
    "    if DEBUG_PLOTS:\n",
    "        plt.title(\"Base (A placed onto canvas)\")\n",
    "        show_image(base[..., ::-1])\n",
    "\n",
    "    blended = poisson_blend(base, warpedB)\n",
    "    if DEBUG_PLOTS:\n",
    "        plt.title(\"Blended\")\n",
    "        show_image(blended[..., ::-1])\n",
    "\n",
    "    return blended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bc3777-d062-4354-ab31-debd8ec0c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "pano12 = stitch_two_poisson(img1, img2, feature, kind)\n",
    "pano12 = crop_to_content(pano12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552305b6-88b3-4be5-bdfa-d7dfabe43e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pano123 = stitch_two_poisson(pano12, img3, feature, kind)\n",
    "pano123 = crop_to_content(pano123)\n",
    "pano123_poisson = pano123.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f844f8-d953-43ab-ba6a-00e236928fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(8, 6))\n",
    "ax = ax.flatten()\n",
    "\n",
    "ax[0].imshow(pano123_simple[..., ::-1])\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(pano123_poisson[..., ::-1])\n",
    "ax[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64abc24e-d06e-4c90-a5ff-c322808f998b",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
